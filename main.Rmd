---
title: "Assignment 2"
author: "Nyasha Mashanda Mshnya010"

output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(shiny)) install.packages("shiny", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(plotly)) install.packages("plotly", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(formattable)) install.packages("formattable", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(rattle)) install.packages("rattle", repos = "http://cran.us.r-project.org")
if(!require(shinythemes)) install.packages("shinythemes", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(rAverage)) install.packages("rAverage", repos = "http://cran.us.r-project.org")
if(!require(caTools)) install.packages("caTools", repos = "http://cran.us.r-project.org")
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org")
library(knitr)
library(cowplot)
if(!require(cowplot)) install.packages("cowplot", repos = "http://cran.us.r-project.org")
if(!require(ROCR)) install.packages("ROCR", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tree)) install.packages("tree", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")

```


# Exploratory Data Analysis

The imported data is made up of 7 columns namely Date, Temeperature, Humidity, Light, CO2, HumidityRatio and Occupancy columns. Inspecting the head and tail of the data showed that the .csv file was imported correctly. The date column shows that the train data was collected between 05 and 09 February 2015. All the columns are of numeric type except for the date column which is in POSIXct format. The occupancy variable has two classes hence it is changed to a factor type. No missing values were found in the training set.



```{r echo=FALSE , include=F, warning=F}
train <- read_csv("occupancy_training.csv")
test <- read_csv("occupancy_testing.csv")

# Inspecting the training set 

names(train)
head(train,5)
tail(train,5)
str(train)

sum(is.na(train))

unique(train$Occupancy)

# Making occupancy a factor
train$Occupancy = as.factor(train$Occupancy)
test$Occupancy = as.factor(test$Occupancy)

str(train)
str(test)

summary(train)

## Boxplots of the variables
par(mfrow=c(2,3))
boxplot(train$Temperature, ylab = "Temperature")
boxplot(train$Humidity, ylab = "Hunidity")
boxplot(train$Light, ylab = "Light")
boxplot(train$CO2, ylab = "CO2 Concentration")
boxplot(train$HumidityRatio, ylab = "Humidity Ratio")

# Outlier detection
outliers_light <- boxplot(train$Light, plot = F)$out
out_lt <- train[which(train$Light %in% outliers_light),]

outliers_co2 <- boxplot(train$CO2, plot = F)$out
out_co2 <- train[which(train$CO2 %in% outliers_co2),]

min_c <- min(out_co2$date)
max <- max(out_co2$date)

outliers_hr <- boxplot(train$HumidityRatio, plot = F)$out
out_hr <- train[which(train$HumidityRatio %in% outliers_hr),]

outliers_hr <- boxplot(train$HumidityRatio, plot = F)$out
out_hr <- train[which(train$HumidityRatio %in% outliers_hr),]

## removing light outliers
df1 <- train[-which(train$Light %in% outliers_light),]

## removing humidity ratio outliers that have zero occupancy
outliers_hr <- boxplot(df1$HumidityRatio, plot = F)$out
out_hr <- train[which(train$HumidityRatio %in% outliers_hr),]

out_hr_0 <- filter(out_hr, Occupancy == "0")
outliers_hr_0 <- out_hr_0$HumidityRatio


df2 <- df1


```


The following plot shows how each variable is changing with respect to time.

```{r fig.cap="Response variables in training set", fig.height=10, fig.width=8, fig.align="center", echo=F}
## All variables before outlier removal
p5 <- ggplot(train, aes(date, Temperature)) + geom_line(colour = "blue")
p6 <- ggplot(train, aes(date, Humidity)) + geom_line(colour = "red")
p7 <- ggplot(train, aes(date, Light)) + geom_line(colour = "green")
p8 <- ggplot(train, aes(date, CO2)) + geom_line(colour = "orange")
p9 <- ggplot(train, aes(date, Occupancy)) + geom_point(size = 0.1, colour = "brown") 

plot_grid(p5,p6,p7,p8,p9,  align = "v", nrow = 5)
```

\pagebreak

## Outlier Detection

The first step that was taken to detect any outliers was plotting a box and whisker diagrams for the Temperature, Humidity, Light, CO2 and HumidityRatio variables as shown below:

```{r fig.cap="Box and Whisker Plots of Training Data Response Variables", echo=F, fig.height= 5 }
par(mfrow=c(2,3))
boxplot(train$Temperature, ylab = "Temperature")
boxplot(train$Humidity, ylab = "Hunidity")
boxplot(train$Light, ylab = "Light")
boxplot(train$CO2, ylab = "CO2")
boxplot(train$HumidityRatio, ylab = "Humidity Ratio")

```

The box and whisker plots show that other variables have no outliers except for Light, C02 and Humidity Ratio. Investigating the CO2 variable shows that CO2 concentration was exceptionally high between 10:40:59 and 22:11:00 on the 9th of February, resulting in 432 outlier readings. It is possible that the C02 concentration was very high due to a high number of people occupying the room since the occupancy status was positive(1) for the greater part of that time as shown in Fig.3 below. The outliers are also present when the occupancy status in negative(0), but during this time the C02 levels were going down hence it is possible that all the people had left the room and the C02 level started decreasing steadily. Therefore, the outliers were considered valid and were kept in the data.

\pagebreak

```{r echo=F, fig.height= 4, fig.width= 6, fig.cap= "CO2 Outliers"}
p1 <- ggplot(out_co2, aes(date, CO2)) + geom_line()
p2 <- ggplot(out_co2, aes(date, Occupancy)) + geom_line()

plot_grid(p1,p2, nrow = 2,  align = "v") ## Check C02 ouliers
```

The light variable has two outliers on 07 February 09:42:00 and 09:42:59 when the room is not occupied. This is very unusual when compared to the maximum light measurements prior to and after the spikes. This was considered an error in reading and since the dataset is very big with more than 6000 rows, the two outliers were removed. 

Humidity ratio has 139 outliers between 15:45:00 and 18:08:00 on the 9th of February. Given that the readings were very high when the room was occupied, the readings were considered valid and were kept. 

## Feature engineering

The imported data includes a date variable that is constantly increasing. This is not useful for model building as the increase is not in any way related to the occupancy status. Therefore, the date variable was used to create the following new variables: num_sec and weekday which indicate the number of seconds from midnight and if the day is a weekday or not, respectively. The new dataset was saved as set_b while the unaltered dataset was saved as set_a.

These datasets will be used to find the best model using each model’s AUC result on the test set.


```{r echo=FALSE}
## Creating various train and test sets

train_a <- df2
test_a <- test

# Create dataset with Weekday and num_sec
train_b <- df2
test_b <- test

train_b$Weekday <- ifelse((wday(df2$date) == 1 | wday(df2$date) == 7), "0", "1")
train_b$Weekday <- as.factor(train_b$Weekday)
test_b$Weekday <- ifelse((wday(test$date) == 1 | wday(test$date) == 7), "0", "1")
test_b$Weekday <- as.factor(test_b$Weekday)

# find num_sec from the start of the day
## This part of the code was taken from https://github.com/LuisM78/Occupancy-detection-data/blob/master/Model%20Development.R

second_day <- function(x) {
        # x is an object in posixct format
        s <- hour(x)*3600+minute(x)*60+second(x)
}
train_b$num_sec <- second_day(train_b$date)
test_b$num_sec <- second_day(test_b$date)

## Re-ordering
train_b <- dplyr::select(train_b, c(1:6,8,9,7))
test_b <- dplyr::select(test_b, c(1:6,8,9,7))


```

# Logistic Regression

The logistic regression model has a binary response given that there are two classes of the occupancy status with “1” meaning occupied and “0” unoccupied.

Because the classes are imbalanced, and assuming both classes are equally important, ROC AUC will be used for performance measurement. Both classes are assumedly important since detecting a true negative will allow the system to save energy while also detecting a true positive will ensure that an automatic system will make room resources (electricity, security) available to the occupants.


```{r echo=FALSE, include=F}
## Without regularization

## Removing date as a variable
mod1a <- glm(Occupancy~.-date,data = train_a, family = binomial)
s1 <- summary(mod1a)


## model with weekday and num_sec
mod1b <- glm(Occupancy~.-date,data = train_b, family = binomial)
s2 <- summary(mod1b)


## Error on training set
pi_hat_1a <- predict(mod1a, type = 'response')
Y_hat_1a <- ifelse(pi_hat_1a >= 0.5, "1", "0")
N <- length(Y_hat_1a)
Error_1a <- sum((Y_hat_1a != train_a$Occupancy))/N

## Error on test set
pi_hat_2a <- predict(mod1a, newdata = test_a, type = 'response')
Y_hat_2a <- ifelse(pi_hat_2a >= 0.5, "1", "0")
N <- length(Y_hat_2a)
Error_2a <- sum((Y_hat_2a != test_a$Occupancy))/N


pred <- prediction(pi_hat_2a, test_a$Occupancy)
perf <- performance(pred, 'tpr', 'fpr') 

# calculate probabilities for TPR/FPR for predictions 
auc_value <- performance(pred, measure = 'auc')@y.values[[1]]


## Make prediction on the with week and num_sec

pi_hat_1b <- predict(mod1b, type = 'response')
Y_hat_1b <- ifelse(pi_hat_1b >= 0.5, "1", "0")
N <- length(Y_hat_1b)
Error_1b <- sum((Y_hat_1b != train_b$Occupancy))/N

pi_hat_2b <- predict(mod1b, newdata = test_b, type = 'response')
Y_hat_2b <- ifelse(pi_hat_2b >= 0.5, "1", "0")
N <- length(Y_hat_2b)
Error_2b <- sum((Y_hat_2b != test_b$Occupancy))/N


pred <- prediction(pi_hat_2b, test_b$Occupancy)
perf <- performance(pred, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 
auc_value <- performance(pred, measure = 'auc')@y.values[[1]]

```


## Logistic Regression Wwthout Regularization

A glm logistic regression model was fitted on training set a, the following results were found:


```{r echo=FALSE,  out.width = '55%', out.height='55%', fig.align="center" }
knitr::include_graphics("Capture 1.png")
```

The results show that Humidity, Light, CO2  and HumidityRatio have a significant relationship with Occupancy, while Temperature has an insignificant relationship. This is based on the p-values. The results also suggest that an increase in humidity ratio makes occupancy less likely, while an increase in the other variables makes occupancy more likely. The auc of the model was found to be 98.84%


The logistic regression model using set b gave the following results:

```{r echo=FALSE, out.width = '55%', out.height='55%', fig.align="center" }
knitr::include_graphics("Capture 2.png")
```

The results for set b show that Temperature, Humidity, Light, CO2 and HumidityRatio have a significant relationship with Occupancy, while Weekday and Num_sec have an insignificant relationship. The results also suggest that an increase in HumidityRatio makes occupancy less likely based on the negative beta value. Increase in reading of other variables make occupancy more likely. The auc of the model was found to be 98.67%

## Logistic Regression with Regularization

### Lasso

Fitting a Lasso model on set_a, gives a lambda.min model that has two variables, Light and CO2 while the lambda.1se model has only the Light variable. The lambda.min model had an AUC of 99.14% while the lambda.1se model had an AUC of 99.03% on the test set. Therefore, for set a, the lambda.min model is the better of the two modelwith Light and CO2 as its predictors.

\pagebreak

```{r echo=FALSE}
## Lasso set_a

X <- data.matrix(train_a[,c(-1,-7)])
Y <- data.matrix(train_a[7])

set.seed(1)
mod_a <- glmnet(X, Y, alpha = 1, nfolds = 10, type.measure = 'class', standardize = T, family = 'binomial')
set.seed(1)
cv <- cv.glmnet(X, Y, alpha = 1, nfolds = 10, type.measure = 'class', standardize = T, family = 'binomial')
```

The misclassification error plot of the Lasso model is shown below.

```{r echo=F, fig.cap="Lasso on set a", fig.height=4, fig.width=6}
plot(cv)
```


```{r echo=FALSE}
c1 <- coef(mod_a, s=cv$lambda.1se)
c2 <- coef(mod_a, s=cv$lambda.min)

# Using min model

new_x <- data.matrix(test_a[,c(-1,-7)])
new_y <- data.matrix(test_a[7])
pi_hat_3 <- predict(mod_a, newx =  new_x, type = 'response', s = cv$lambda.min)
pred3 <- prediction(pi_hat_3, test_a$Occupancy)
perf3 <- performance(pred3, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred3, measure = 'auc')@y.values[[1]]

Y_hat_3 <- ifelse(pi_hat_3 >= 0.5, "1", "0")
cm <- confusionMatrix(as.factor(Y_hat_3), test_a$Occupancy)


pred <- prediction(pi_hat_3, test_a$Occupancy)
perf <- performance(pred, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred, measure = 'auc')@y.values[[1]]




## Using 1se model

pi_hat_4 <- predict(mod_a, newx =  new_x, type = 'response', s = cv$lambda.1se)
pred4 <- prediction(pi_hat_4, test_a$Occupancy)
perf4 <- performance(pred4, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred4, measure = 'auc')@y.values[[1]]

Y_hat_4 <- ifelse(pi_hat_4 >= 0.5, "1", "0")
cm <- confusionMatrix(as.factor(Y_hat_4), test_a$Occupancy)


pred <- prediction(pi_hat_4, test_a$Occupancy)
perf <- performance(pred, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred, measure = 'auc')@y.values[[1]]


## might have to use the training set to choose the model not test set.

```


```{r echo=FALSE}

## Lasso Set_b

X <- data.matrix(train_b[,c(-1,-9)])
Y <- data.matrix(train_b[9])

set.seed(1)
mod2 <- glmnet(X, Y, alpha = 1, nfolds = 10, type.measure = 'class', standardize = T, family = 'binomial')
cv <- cv.glmnet(X, Y, alpha = 1, nfolds = 10, type.measure = 'class', standardize = T, family = 'binomial')

```


Fitting a Lasso model on set_b gave a lambda.min model that has three variables, Light, Weekday and CO2 while the lambda.1se model had only the Light variable. The lambda.min model had an AUC of 99.14% while the lambda.1se model had an AUC of 99.03% on the test set. Therefore, using set a, the lambda.min model is the better of the two model with Light, Weekday and CO2 as its predictors.

```{r echo=F, fig.cap="Lasso on set b"}
#plot(cv)
```


```{r echo=FALSE, include=F}


c3 <- coef(mod2, s=cv$lambda.1se)
c4 <- coef(mod2, s=cv$lambda.min)

# Using min model

new_x <- data.matrix(test_b[,c(-1,-9)])
new_y <- data.matrix(test_b[9])
pi_hat_3 <- predict(mod2, newx =  new_x, type = 'response', s = cv$lambda.min)
pred3 <- prediction(pi_hat_3, test$Occupancy)
perf3 <- performance(pred3, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

performance(pred3, measure = 'auc')@y.values[[1]]

Y_hat_3 <- ifelse(pi_hat_3 >= 0.5, "1", "0")
cm <- confusionMatrix(as.factor(Y_hat_3), test_b$Occupancy)


pred <- prediction(pi_hat_3, test_b$Occupancy)
perf <- performance(pred, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred, measure = 'auc')@y.values[[1]]


## Using 1se model

pi_hat_4 <- predict(mod2, newx =  new_x, type = 'response', s = cv$lambda.1se)
pred4 <- prediction(pi_hat_4, test_b$Occupancy)
perf4 <- performance(pred4, 'tpr', 'fpr') 
# calculate probabilities for TPR/FPR for predictions 
auc_value <- performance(pred4, measure = 'auc')@y.values[[1]]

```


```{r echo=FALSE}
## Ridge model set_a

X <- data.matrix(train_a[,c(-1,-7)])
Y <- data.matrix(train_a[7])

mod_a <- glmnet(X, Y, alpha = 0, nfolds = 10, type.measure = 'class', standardize = T, family = 'binomial')
set.seed(1)
cv <- cv.glmnet(X, Y, alpha = 0, nfolds = 10, type.measure = 'class', standardize = T, family = 'binomial')
```

## Ridge Regression

The ridge regression model for set a had an AUC of 99.07% for the lambda.min model while the lambda.1se model had an AUC of 99.07%. The misclassification error plot of the lambda.min model is shown below.

\pagebreak


```{r echo=F, fig.cap="Ridge on set a", fig.height=4, fig.width=6}
plot(cv)
```

```{r echo=FALSE}

c5 <- coef(mod_a, s=cv$lambda.1se)
c6 <- coef(mod_a, s=cv$lambda.min)

## Using min model
new_x <- data.matrix(test_a[,c(-1,-7)])
new_y <- data.matrix(test_a[7])
pi_hat_5 <- predict(mod_a, newx =  new_x, type = 'response', s = cv$lambda.min)
pred5 <- prediction(pi_hat_5, test_a$Occupancy)
perf5 <- performance(pred5, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred5, measure = 'auc')@y.values[[1]]

Y_hat_5 <- ifelse(pi_hat_5 >= 0.5, "1", "0")
cm <- confusionMatrix(as.factor(Y_hat_5), test_a$Occupancy)

## Using 1se model

new_x <- data.matrix(test_a[,c(-1,-7)])
new_y <- data.matrix(test_a[7])
pi_hat_6 <- predict(mod_a, newx =  new_x, type = 'response', s = cv$lambda.1se)
pred6 <- prediction(pi_hat_6, test$Occupancy)
perf6 <- performance(pred6, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred6, measure = 'auc')@y.values[[1]]

Y_hat_6 <- ifelse(pi_hat_6 >= 0.5, "1", "0")
cm <- confusionMatrix(as.factor(Y_hat_6), test_b$Occupancy)

## might have to use the training set to choose the model not test set.

```

The ridge regression model for set b had an auc of 99.08% for the lamda.min model while the lambda.1se model has an auc of 99.07%. Therefore, feature engineering does not have a significant change on the model performance for ridge regrssion.




```{r echo=FALSE}
## Ridge model set_b

X <- data.matrix(train_b[,c(-1,-9)])
Y <- data.matrix(train_b[9])

mod_b <- glmnet(X, Y, alpha = 0, nfolds = 10, type.measure = 'class', standardize = T, family = 'binomial')
set.seed(1)
cv <- cv.glmnet(X, Y, alpha = 0, nfolds = 10, type.measure = 'class', standardize = T, family = 'binomial')
```

```{r include=F, fig.cap="Ridge on set b"}
#plot(cv)
```

```{r echo=FALSE}

c7 <- coef(mod_b, s=cv$lambda.1se)
c8 <- coef(mod_b, s=cv$lambda.min)

## Using min model
new_x <- data.matrix(test_b[,c(-1,-9)])
new_y <- data.matrix(test_b[9])
pi_hat_5 <- predict(mod_b, newx =  new_x, type = 'response', s = cv$lambda.min)
pred5 <- prediction(pi_hat_5, test_b$Occupancy)
perf5 <- performance(pred5, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred5, measure = 'auc')@y.values[[1]]

Y_hat_5 <- ifelse(pi_hat_5 >= 0.5, "1", "0")
cm <- confusionMatrix(as.factor(Y_hat_5), test_b$Occupancy)

## Using 1se model

new_x <- data.matrix(test_b[,c(-1,-9)])
new_y <- data.matrix(test_b[9])
pi_hat_6 <- predict(mod_b, newx =  new_x, type = 'response', s = cv$lambda.1se)
pred6 <- prediction(pi_hat_6, test$Occupancy)
perf6 <- performance(pred6, 'tpr', 'fpr') # calculate probabilities for TPR/FPR for predictions 

auc_value <- performance(pred6, measure = 'auc')@y.values[[1]]

Y_hat_6 <- ifelse(pi_hat_6 >= 0.5, "1", "0")
cm <- confusionMatrix(as.factor(Y_hat_6), test_b$Occupancy)

## might have to use the training set to choose the model not test set.

```

## Conclusion


```{r echo=F}
Regular <- read_excel("Regular.xlsx")
knitr::kable(Regular)
```

The logistic regression results suggest that the best models are the lasso lambda.min models using either set a or set b. The model from set a is chosen as it gives the same performance of 99.14% AUC but using only two variables, Light and CO2.

The parameters of the chosen model are shown below:

```{r echo=FALSE, fig.cap="", out.width = '35%', out.height='35%', fig.align="center" }
knitr::include_graphics("Capture 3.png")

```



```{r echo=FALSE}

## Classification Tree without Weekday and num_sec
set.seed(1)
tree_a <- tree(Occupancy ~ .-date, data = train_a) 
s1 <- summary(tree_a) 

cv_tree_a <- cv.tree(tree_a , FUN = prune.misclass) #use classification error rate for pruning
best_t_a <- cv_tree_a$size[which.min(cv_tree_a$dev)] #The minimum CV Error

```

\pagebreak

# Classification Trees

A classification tree was fit on the training set a data and then pruned to find a model with the best bias-variance trade-off. The best tree had 5 terminal nodes as shown below.


```{r echo=F, fig.cap="Classification tree error on set a", fig.height=4, fig.width=6}
plot(cv_tree_a$size, cv_tree_a$dev, type='o', pch = 16, col = 'navy', lwd = 2,
     xlab='Number of terminal nodes', ylab='CV error')
```

The pruned tree is shown in Fig 7.

```{r echo=FALSE}
# (3) Prune the tree
set.seed(1)
pr_tree_a <- prune.misclass(tree_a, best = best_t_a)
```

```{r echo=F, fig.cap="Pruned tree set a", fig.height=3, fig.width=6}
plot(pr_tree_a, type = 'uniform')
text(pr_tree_a, pretty = 0)

```

The tree shows that Light is the best variable to determining occupancy. The deviance falls from 7528 to 365.125 when Light was used for the first split. If the light measurement is less than 365.125 then the occupancy is predicted to be negative. The occupancy is also considered negative when light < 365.125, temperature > 22.2113 and CO2 level is less than 893.125.

The room is considered occupied if Light > 365.125 and Temperature < 22.2113 or Light > 365.125, Temperature > 22.2113, CO2 levels greater than 893.125 and Temperature less than 22.6417.

As a result, the classification tree on set a used Light, Temperature and CO2 to determine occupancy. The AUC of the pruned tree is 98.49%

```{r echo=FALSE, include=F}
pred_a <- predict(pr_tree_a, test_a, type = 'class') #type argument nb for classification!


PredWithProbabilities <- predict(pr_tree_a, test_a, type = 'vector')
auc <- auc(test_a$Occupancy, PredWithProbabilities[,2])

```

\pagebreak

# Bagging/Random Forests Set a

To find the best bag model, the number of tree was set to 5000 and Temperature, Humidity, HumidityRatio, Light and CO2 variables were considered on each split of every tree.

```{r echo=FALSE, include=F}
## Using bagging/random forests without num_sec and Weekday

data_comb <- rbind(train_a[,-1], test_a[,-1])


## Train/test split
set.seed(1)

## Fit a bagged trees model
bag1 <- randomForest(Occupancy ~ ., data = data_comb[1:6681,],
                    mtry = ncol(data_comb) - 1, #for bagging, use all predictors
                    ntree = 5000, #number of trees
                    importance = TRUE, #keep track of reduction in loss function
                    do.trace = 100)  #print out regular progress
                    # and more (see ?randomForest)

```


The results showed that the Light variable is very important while the CO2, Temperature, Humidity and Humidity Ratio where considered of little importance in determining the occupancy status. The AUC was found to be 98.76%. The OOB error plot shows that 500 trees would be enough to give a low error with little flactuations as shown in Fig. 8.

```{r echo=F, fig.height= 4, fig.width=5}

bag_varimp <- randomForest::importance(bag1, type=2)
bag_varimp <- bag_varimp[order(bag_varimp, decreasing=FALSE),]
barplot(bag_varimp, horiz = T, col = 'navy', las=1,
        xlab = 'Mean decrease in Gini index', cex.lab = 1.5, cex.axis = 1.2,
        main = 'Occupancy', cex.main = 1.8, cex.names = 1.2)
```

\pagebreak

## Random Forest

```{r echo=FALSE,include=F}

## Fit a Random Forest
set.seed(1)
rf <- randomForest(Occupancy ~ ., data = data_comb[1:6681,],
                           ntree = 5000,
                           mtry = floor(sqrt(ncol(data_comb[,-6]))),
                           importance = TRUE,
                           do.trace = 100)
# For classification tree, default mtry = floor(sqrt(ncol(x)))
rf


```


The random forest model results show that Light is the most important variable followed by CO2 and Temperature in that order in determining occupancy status. HumidityRatio and Humidity were considered to be of little importance. Furthermore, 600 trees were found to be enough to minimize to minimise OOB error as shown in Fig. 8. The AUC was found to be 98.63%.

```{r echo=F, fig.height= 5}
rf_varimp <- randomForest::importance(rf, type=2)
rf_varimp <- rf_varimp[order(rf_varimp, decreasing=FALSE),]
barplot(rf_varimp, horiz = T, col = 'navy', las=1,
        xlab = 'Mean decrease in Gini index', cex.lab = 1.5, cex.axis = 1.2,
        main = 'Occupancy', cex.main = 1.8, cex.names = 1.2)
```

```{r echo=F, fig.cap="Bagging and Random Forests models"}
# compare OOB errors:
plot(rf$err.rate[, 'OOB'], type = 's', xlab = 'Number of trees', ylab = 'OOB error')
lines(bag1$err.rate[, 'OOB'], col = 'red', type = 's')
legend('topright', legend = c('Bagging', 'Random Forest'), col = c('red', 'black'), lwd = 2)


```

```{r echo=FALSE, include=F}
## Time to use both models for prediction

bag_pred <- predict(bag1, newdata = data_comb[6682:9346,], type = "response") #see
rf_pred <- predict(rf, newdata = data_comb[6682:9346,])

ytest <- data_comb[6682:9346,6]


PredWithProbabilities <- predict(bag1, data_comb[6682:9346,], type = 'prob')
auc <- auc(ytest$Occupancy, PredWithProbabilities[,2])



PredWithProbabilities <- predict(rf, newdata = data_comb[6682:9346,], type = 'prob')
auc <- auc(ytest$Occupancy, PredWithProbabilities[,2])



```

\pagebreak

# Gradient Boosted Trees

In gradient boosted trees using set a, the learning rate was varied between 0.5, 0.1 and 0.05. The interaction depth was varied between 1, 2, 3 and 4. The number of trees was varied between 20 and 500  trees (in increments of 20) and the minimum number of nodes was set to 10. The best model was found at 500 trees, a learning rate of 0.1 and an interaction depth of 4 with an AUC 97.52%

```{r echo=FALSE, include=F}
# Boosted Forests without num_sec and Weekday

# changing the occupancy to integer

df4 <- train_a
df4$Occupancy <- as.integer(df4$Occupancy) - 1

# Using caret

d <- train_a
c <- test_a

d$Occupancy <- ifelse(d$Occupancy==1,'yes','no')
d$Occupancy <- as.factor(d$Occupancy)

c$Occupancy <- ifelse(c$Occupancy==1,'yes','no')
c$Occupancy <- as.factor(c$Occupancy)

objControl <- trainControl(method='cv', number=5, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)

gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3,4),
                        n.trees = (1:25)*20,
                        shrinkage = c(0.5, 0.1, 0.05),
                        n.minobsinnode = 10)


set.seed(1)
objModel <- train(d[,c(-1,-7)], d$Occupancy,
                  method='gbm',
                  trControl=objControl,
                  tuneGrid = gbmGrid,
                  metric = "ROC",
                  preProc = c("center", "scale"),
                  bag.fraction = 1)

```

```{r echo=F, fig.cap="Variable Importance gbm", fig.height=4, fig.width=6}
plot(varImp(objModel))

```

The variable importance results show that Light is considered very important in determining occupancy while Temperature, Humidity, Humidity Ratio and CO2 were found to be of little importance.

```{r echo=F, include=F}
#Evaluate the model
predictions <- predict(object=objModel, c[,c(-1,-7)], type='prob')
auc <- roc(ifelse(c[,7] =="yes",1,0), predictions[[2]])

```

\pagebreak

## Classification Trees set b

A classification tree was fit on training set b and then pruned to find a model with the best bias-variance trade-off. The best pruned tree model had 4 terminal nodes as shown below:


```{r echo=FALSE, include=F}
## Classification Tree with Weekday and num_sec

set.seed(1)
tree_b <- tree(Occupancy ~ .-date, data = train_b)
summary(tree_b)

set.seed(1)
cv_tree_b <- cv.tree(tree_b , FUN = prune.misclass) #use classification error rate for pruning
best_t_b <- cv_tree_b$size[which.min(cv_tree_b$dev)] #The minimum CV Error
```


```{r echo=F, fig.cap="Classification tree error on set b", fig.height=4, fig.width=6}

plot(cv_tree_b$size, cv_tree_b$dev, type='o', pch = 16, col = 'navy', lwd = 2,
     xlab='Number of terminal nodes', ylab='CV error')
```



```{r , include=F}
# (3) Prune the tree
set.seed(1)
pr_tree_b <- prune.misclass(tree_b, best = best_t_b)
```

The pruned tree is shown in Fig 11.

```{r echo=F, fig.cap="Pruned tree set b", fig.height=4, fig.width=6}
plot(pr_tree_b, type = 'uniform')
text(pr_tree_b, pretty = 0)
```

The plot shows that Light is the best variable to determine occupancy. The deviance falls from 7528 to 365.125 when Light was used for the first split. If the light measurement is less than 365.125, then the occupancy is considered to be negative. The occupancy is also considered negative when light > 365.125 temperature > 22.2113 and Num_sec is less than 51119.

The room is considered occupied if Light > 365.125 and Temperature < 22.2113 or Light > 365.125, Temperature > 22.2113 and Num_sec greater than 51119. The AUC of the pruned model was 98.28%.

```{r echo=FALSE,  include=F}

pred_b <- predict(pr_tree_b, test_b, type = 'class') #type argument nb for classification!

confusionMatrix(pred_b, test_b$Occupancy)

PredWithProbabilities <- predict(pr_tree_b, test_b, type = 'vector')
auc <- auc(test_b$Occupancy, PredWithProbabilities[,2])

```

\pagebreak

# Bagging/Random Forests Set b

A bag model was fitted on set b with the number of trees set to 5000. The Temperature, Humidity, Humidity Ratio, Light, CO2, Weekday and Numsec variables were considered on each split.

```{r echo=FALSE, include=F}
## Using bagging/random forests without num_sec and Weekday
data_comb <- rbind(train_b[,-1], test_b[,-1])


## Train/test split
set.seed(1)

## Fit a bagged trees model
bag1 <- randomForest(Occupancy ~ ., data = data_comb[1:6681,],
                    mtry = ncol(data_comb) - 1, #for bagging, use all predictors
                    ntree = 5000, #number of trees
                    importance = TRUE, #keep track of reduction in loss function
                    do.trace = 100)  #print out regular progress
                    # and more (see ?randomForest)
bag1

## Choose number of trees:
head(bag1$err.rate)
plot(bag1$err.rate[, 'OOB'], type = 's', xlab = 'Number of trees', ylab = 'OOB error')


## Variable importance plot
varImpPlot(bag1, type = 2) #type=2: Reduction in gini index
# see ?importance

```

```{r echo=F, fig.height=4, fig.width=6}
bag_varimp <- randomForest::importance(bag1, type=2)
bag_varimp <- bag_varimp[order(bag_varimp, decreasing=FALSE),]
barplot(bag_varimp, horiz = T, col = 'navy', las=1,
        xlab = 'Mean decrease in Gini index', cex.lab = 1.5, cex.axis = 1.2,
        main = 'Occupancy', cex.main = 1.8, cex.names = 1.2)
```

The results showed that the Light variable is very important while the CO2, Temperature, Num_sec, Humidity, Humidity Ratio and Weekday are of little importance in determining the occupancy status. The OOB error plot shows that 500 trees would be enough to minimize the OOB error as shown in Fig. 12. The AUC was found to be 83.53%.


```{r echo=FALSE , include=F}

## Fit a Random Forest
set.seed(1)
rf <- randomForest(Occupancy ~ ., data = data_comb[1:6681,],
                           ntree = 5000,
                           mtry = floor(sqrt(ncol(data_comb[,-8]))),
                           importance = TRUE,
                           do.trace = 100)

rf
```

\pagebreak

## Random Forest

The random forest model considered two predictors on each split.

```{r echo=F, fig.height=4, fig.width=6}
rf_varimp <- randomForest::importance(rf, type=2)
rf_varimp <- rf_varimp[order(rf_varimp, decreasing=FALSE),]
barplot(rf_varimp, horiz = T, col = 'navy', las=1,
        xlab = 'Mean decrease in Gini index', cex.lab = 1.5, cex.axis = 1.2,
        main = 'Occupancy', cex.main = 1.8, cex.names = 1.2)
```

The results from the variable importance plot shows that Light is the most important variable followed by CO2, Temperature and Num_sec in that order. Humidity Ratio, Weekday and Humidity were considered to be of little importance. Furthermore, 300 trees will be enough to minimize the OOB error as shown in Fig. 12. The AUC was found to be 80.72%.

```{r echo=F, fig.cap="Bagging and Random Forests models"}
# compare OOB errors:
plot(rf$err.rate[, 'OOB'], type = 's', xlab = 'Number of trees', ylab = 'OOB error')
lines(bag1$err.rate[, 'OOB'], col = 'red', type = 's')
legend('topright', legend = c('Bagging', 'Random Forest'), col = c('red', 'black'), lwd = 2)
#No need for more trees for either model
```


```{r echo=FALSE, include=F}

bag_pred <- predict(bag1, newdata = data_comb[6682:9346,], type = "response") #see ?predict.randomForest
rf_pred <- predict(rf, newdata = data_comb[6682:9346,])

ytest <- data_comb[6682:9346,8]

PredWithProbabilities <- predict(bag1, newdata = data_comb[6682:9346,], type = "prob") #see ?predict.randomForest
auc <- auc(test_b$Occupancy, PredWithProbabilities[,2])
auc


PredWithProbabilities <- predict(rf, newdata = data_comb[6682:9346,], type = "prob")
auc <- auc(test_b$Occupancy, PredWithProbabilities[,2])
auc


```

\pagebreak

# Gradient Boosted Trees

For gradient boost, the learning rate was varied between 0.5, 0.1 and 0.05. The interaction depth included the following: 1, 2, 3 and 4. Number of trees 20 to 500 (in 20 increments) trees and the minimum number of nodes was set to 10. The best model was found at 260 trees, a learning rate of 0.5 and interaction depth 2.


```{r echo=FALSE, include=F}

# changing the occupancy to integer

df4 <- train_b
df4$Occupancy <- as.integer(df4$Occupancy) - 1

# Using caret

d <- train_b
c <- test_b

d$Occupancy <- ifelse(d$Occupancy==1,'yes','no')
d$Occupancy <- as.factor(d$Occupancy)

c$Occupancy <- ifelse(c$Occupancy==1,'yes','no')
c$Occupancy <- as.factor(c$Occupancy)

objControl <- trainControl(method='cv', number=5, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)

gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3,4),
                        n.trees = (1:25)*20,
                        shrinkage = c(0.5, 0.1, 0.05),
                        n.minobsinnode = 10)

set.seed(1)
objModel <- train(d[,c(-1,-9)], d$Occupancy,
                  method='gbm',
                  trControl=objControl,
                  tuneGrid = gbmGrid,
                  metric = "ROC",
                  bag.fraction = 1)
summary(objModel)
objModel$finalModel

```

```{r, fig.cap="Variable Importance gbm", echo=F }
plot(varImp(objModel))
```
The results show that Light is considered very important in determining occupancy while Temperature, Humidity, Humidity Ratio, Weekday, Num_sec and CO2 were found to be of little importance. The auc was found to be 97.33%.


```{r echo=F, include=F}
#Evaluate the model
predictions <- predict(object=objModel, c[,c(-1,-9)], type='prob')
auc <- roc(ifelse(c[,9] =="yes",1,0), predictions[[2]])


```

### Trees Results

The tree results are summarised below.

```{r echo=F}
Book1 <- read_excel("Book1.xlsx")
knitr::kable(Book1, digits = 5)
```

## Conclusion

The lasso regression model is the chosen model as it has the highest AUC of 99.14% using Light and CO2 as the predictor variables
